# Pipeline Batch Bovespa - POS Tech MLE

API para coleta dos dados da B3: "https://sistemaswebb3-listados.b3.com.br/indexProxy/indexCall/GetPortfolioDay"
- Fornece dados da carteeira diÃ¡ria da B3.

## ğŸ“‹ SumÃ¡rio

- [ğŸ“‹ DescriÃ§Ã£o](#-descriÃ§Ã£o)
- [ğŸ—ï¸ Arquitetura](#ï¸-arquitetura)
- [ğŸš€ Funcionalidades](#-funcionalidades)
- [ğŸ› ï¸ Tecnologias Utilizadas](#ï¸-tecnologias-utilizadas)
- [ğŸ“¦ InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ğŸš€ Como Usar](#-como-usar)
- [ğŸ“ Estrutura do Projeto](#-estrutura-do-projeto)
- [ğŸ”§ Componentes Principais](#-componentes-principais)
- [ğŸ”„ Fluxo de Desenvolvimento](#-fluxo-de-desenvolvimento)
- [ğŸ“Š Dados Processados](#-dados-processados)
- [ğŸ”§ Desenvolvimento](#-desenvolvimento)
- [ğŸ“ ConfiguraÃ§Ãµes](#-configuraÃ§Ãµes)
- [ğŸ¤ ContribuiÃ§Ã£o](#-contribuiÃ§Ã£o)
- [ğŸ“ Suporte](#-suporte)
- [ğŸ“š DocumentaÃ§Ã£o Adicional](#-documentaÃ§Ã£o-adicional)
- [ğŸ”„ Status do Projeto](#-status-do-projeto)

## ğŸ“‹ DescriÃ§Ã£o

O pipeline de dados B3 Ã© uma soluÃ§Ã£o completa para coleta, processamento, transformaÃ§Ã£o e armazenamento de dados financeiros do Ibovespa. O sistema foi desenvolvido seguindo uma arquitetura em camadas (Bronze e Silver) na AWS, fornecendo insights valiosos sobre o mercado financeiro brasileiro.

## ğŸ—ï¸ Arquitetura

### Camadas de Dados
- **Bronze (Raw)**: Dados brutos exatamente como recebidos da API B3
- **Silver (Refined)**: Dados processados e separados (header e ativos)

### Componentes AWS
- **S3**: Armazenamento de dados em camadas
- **AWS Glue**: ETL e catalogaÃ§Ã£o de dados
- **AWS Lambda**: AutomaÃ§Ã£o de pipeline
- **AWS Athena**: Consultas SQL nos dados
- **AWS QuickSight**: VisualizaÃ§Ã£o e BI (Opcional)

## ğŸš€ Funcionalidades

- **Coleta AutomÃ¡tica**: IntegraÃ§Ã£o com APIs da B3 para coleta de dados do Ibovespa
- **Processamento ETL**: TransformaÃ§Ã£o e limpeza de dados via AWS Glue
- **Armazenamento EscalÃ¡vel**: Sistema de armazenamento em camadas no S3
- **AutomaÃ§Ã£o**: Pipeline automatizado com Lambda e S3 Events
- **AnÃ¡lise**: Consultas SQL via Athena e visualizaÃ§Ãµes no QuickSight
- **ValidaÃ§Ã£o**: Modelos Pydantic para validaÃ§Ã£o de dados

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.12**: Linguagem principal do projeto
- **Pandas**: ManipulaÃ§Ã£o e anÃ¡lise de dados
- **Boto3**: IntegraÃ§Ã£o com serviÃ§os AWS
- **Pydantic**: ValidaÃ§Ã£o de dados e configuraÃ§Ãµes
- **FastParquet**: Processamento de arquivos Parquet
- **Requests**: RequisiÃ§Ãµes HTTP para API B3
- **PyYAML**: Leitura de arquivos de configuraÃ§Ã£o YAML

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.12 ou superior
- pip (gerenciador de pacotes Python)
- AWS CLI configurado
- Acesso aos serviÃ§os AWS (S3, Glue, Lambda, Athena)

### ConfiguraÃ§Ã£o do Ambiente

1. **Clone o repositÃ³rio**:
```bash
git clone <url-do-repositorio>
cd b3_data_pipeline
```

2. **Crie um ambiente virtual**:
```bash
python -m venv venv
```

3. **Ative o ambiente virtual**:

**Windows**:
```bash
venv\Scripts\activate
```

**Linux/Mac**:
```bash
source venv/bin/activate
```

4. **Instale as dependÃªncias**:
```bash
pip install -r requirements.txt
```

### ConfiguraÃ§Ã£o AWS

Configure suas credenciais AWS usando:

```bash
# Configurar credenciais AWS
aws configure

# Ou configurar um perfil especÃ­fico
aws configure --profile b3-pipeline
```

**InformaÃ§Ãµes necessÃ¡rias:**
- AWS Access Key ID
- AWS Secret Access Key  
- Default region (ex: us-east-1)
- Default output format (json)

## ğŸš€ Como Usar

### ExecuÃ§Ã£o Local

```bash
# Ativar ambiente virtual
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Execute sempre a partir da raiz do projeto:
python -m src.main
```

### Estrutura de Dados Gerados

ApÃ³s a execuÃ§Ã£o, os dados sÃ£o organizados em:

```
data/
â”œâ”€â”€ bronze/                    # Dados brutos (Parquet)
â”‚   â””â”€â”€ YYYY-MM-DD/
â”‚       â””â”€â”€ dados_originais.parquet  # Header + Results juntos
â””â”€â”€ raw/                      # Dados originais (JSON)
    â””â”€â”€ YYYY-MM-DD.json       # Resposta completa da API
```

## ğŸ“ Estrutura do Projeto

```
b3_data_pipeline/
â”œâ”€â”€ ğŸ“„ README.md                    # DocumentaÃ§Ã£o principal
â”œâ”€â”€ ğŸ“„ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ fluxo de desenvolvimento.txt # Guia sequencial de implementaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ arquitetura_aws.txt          # Diagrama da arquitetura AWS
â”œâ”€â”€ ğŸ“ venv/                       # Ambiente virtual Python
â”œâ”€â”€ ğŸ“ src/                        # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ğŸ“ collectors/             # Coletores de dados
â”‚   â”‚   â”œâ”€â”€ b3_scraper.py         # MÃ³dulo principal de coleta
â”‚   â”‚   â””â”€â”€ api_client.py         # Cliente HTTP para APIs
â”‚   â”œâ”€â”€ ğŸ“ models/                 # Modelos de dados Pydantic
â”‚   â”‚   â””â”€â”€ ibov.py               # Modelos para dados do Ibovespa
â”‚   â”œâ”€â”€ ğŸ“ storage/                # Camada de armazenamento
â”‚   â”‚   â”œâ”€â”€ save_raw.py           # Salvar dados brutos em Parquet
â”‚   â”‚   â”œâ”€â”€ local_storage.py      # Armazenamento local
â”‚   â”‚   â””â”€â”€ s3_storage.py         # IntegraÃ§Ã£o com AWS S3
â”‚   â”œâ”€â”€ ğŸ“ utils/                  # UtilitÃ¡rios gerais
â”‚   â”‚   â”œâ”€â”€ config.py             # Gerenciamento de configuraÃ§Ãµes
â”‚   â”‚   â””â”€â”€ helpers.py            # FunÃ§Ãµes auxiliares
â”‚   â””â”€â”€ main.py                   # Script principal de execuÃ§Ã£o
â”œâ”€â”€ ğŸ“ data/                       # Dados processados localmente
â”‚   â”œâ”€â”€ ğŸ“ bronze/                # Camada Bronze - Dados brutos
â”‚   â””â”€â”€ ğŸ“ raw/                   # Dados originais da API (JSON)
â”œâ”€â”€ ğŸ“ config/                     # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ config.yaml               # ConfiguraÃ§Ãµes do projeto
â””â”€â”€ ğŸ“„ test_api_dates.py          # Script de teste de datas
```

## ğŸ”§ Componentes Principais

### **1. `src/collectors/`**
- **`b3_scraper.py`**: MÃ³dulo principal que faz requisiÃ§Ãµes HTTP para a API da B3, obtendo dados do Ibovespa
- **`api_client.py`**: Cliente HTTP reutilizÃ¡vel com configuraÃ§Ãµes de timeout e headers

### **2. `src/models/`**
- **`ibov.py`**: Modelos Pydantic para validaÃ§Ã£o da estrutura dos dados recebidos da API (pages, header e results)

### **3. `src/storage/`**
- **`save_raw.py`**: FunÃ§Ã£o principal para salvar dados brutos em formato Parquet (pages + header + results juntos)
- **`local_storage.py`**: OperaÃ§Ãµes de arquivo local
- **`s3_storage.py`**: Upload de dados para AWS S3

### **4. `src/utils/`**
- **`config.py`**: Gerenciamento de configuraÃ§Ãµes do projeto
- **`helpers.py`**: FunÃ§Ãµes auxiliares reutilizÃ¡veis

### **5. `src/main.py`**
- Script principal que orquestra a coleta, validaÃ§Ã£o e salvamento dos dados

## ğŸ”„ Fluxo de Desenvolvimento

O projeto segue um fluxo de desenvolvimento sequencial conforme documentado em `fluxo de desenvolvimento.txt`:

### **Tarefa 1: ConfiguraÃ§Ã£o do Ambiente Local** âœ…
- Python 3.12 instalado
- Ambiente virtual configurado
- DependÃªncias instaladas
- AWS CLI configurado

### **Tarefa 2: Desenvolvimento do CÃ³digo de ExtraÃ§Ã£o e Salvamento Local** âœ…
- MÃ³dulo `b3_scraper.py` implementado
- Modelos Pydantic em `ibov.py` definidos
- FunÃ§Ã£o `save_raw.py` implementada
- Script principal `main.py` criado

### **Tarefa 3: Teste e GeraÃ§Ã£o Inicial de Dados Locais** âœ…
- Script `main.py` funcional
- Dados sendo gerados em `data/bronze/` e `data/raw/`

### **PrÃ³ximas Tarefas:**
- **Tarefa 4**: ConfiguraÃ§Ã£o inicial na AWS - S3 (Camada Bronze)
- **Tarefa 5**: IngestÃ£o manual de dados para o S3
- **Tarefa 6**: ConfiguraÃ§Ã£o do AWS Glue (FunÃ§Ãµes IAM)
- **Tarefa 7**: Desenvolvimento e execuÃ§Ã£o do AWS Glue Job
- **Tarefa 8**: VerificaÃ§Ã£o do CatÃ¡logo de Dados e teste de consultas (Athena)
- **Tarefa 9**: ConfiguraÃ§Ã£o e desenvolvimento da automaÃ§Ã£o (Lambda e S3 Events)
- **Tarefa 10**: Teste da automaÃ§Ã£o de ponta a ponta
- **Tarefa 11**: ConfiguraÃ§Ã£o de Business Intelligence (AWS QuickSight)

## ğŸ“Š Dados Processados

### Formato dos Dados

Os dados sÃ£o coletados da API da B3 e incluem:

- **Header**: InformaÃ§Ãµes sobre a data de referÃªncia e paginaÃ§Ã£o
- **Results**: Lista de ativos do Ibovespa com informaÃ§Ãµes como:
  - CÃ³digo do ativo
  - Nome da empresa
  - Tipo de ativo
  - ParticipaÃ§Ã£o no Ã­ndice

### Estrutura de Arquivos

```
data/
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ 2025-07-08/
â”‚       â””â”€â”€ dados_originais.parquet  # Header + Results juntos
â””â”€â”€ raw/
    â”œâ”€â”€ 2025-06-30.json
    â”œâ”€â”€ 2025-07-01.json
    â”œâ”€â”€ 2025-07-02.json
    â””â”€â”€ ... (arquivos JSON por data)
```

## ğŸ”§ Desenvolvimento

### Adicionando Novas Funcionalidades

1. Crie uma nova branch para sua feature
2. Implemente a funcionalidade seguindo os padrÃµes do projeto
3. Adicione testes unitÃ¡rios
4. Atualize a documentaÃ§Ã£o
5. FaÃ§a o merge apÃ³s revisÃ£o

### Executando Testes

```bash
# Teste da API de datas
python test_api_dates.py
```

## ğŸ“ ConfiguraÃ§Ãµes

### Arquivo `config/config.yaml`

```yaml
aws:
  region: us-east-1
  bucket_name: b3-techchallenge-fase2-andrea
```

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

## ğŸ“ Suporte

Para suporte e dÃºvidas:

- Abra uma issue no GitHub
- Consulte o arquivo `fluxo de desenvolvimento.txt` para orientaÃ§Ãµes
- Verifique a documentaÃ§Ã£o da API B3

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [Fluxo de Desenvolvimento](fluxo de desenvolvimento.txt)
- [Arquitetura AWS](arquitetura_aws.txt)
- [Estrutura do Projeto](estrutura do projeto.txt)
- [DocumentaÃ§Ã£o da API B3](https://www.b3.com.br/pt_br/market-data-e-indices/servicos-de-dados/market-data/)

## ğŸ”„ Status do Projeto

### âœ… ConcluÃ­do
- ConfiguraÃ§Ã£o do ambiente local
- Desenvolvimento dos mÃ³dulos de coleta
- ImplementaÃ§Ã£o dos modelos de dados
- Sistema de armazenamento local
- Script principal funcional

### ğŸš§ Em Desenvolvimento
- ConfiguraÃ§Ã£o AWS
- ImplementaÃ§Ã£o do pipeline ETL
- AutomaÃ§Ã£o com Lambda

### ğŸ“‹ PrÃ³ximos Passos
- Deploy na AWS
- ConfiguraÃ§Ã£o do Glue Job
- ImplementaÃ§Ã£o da automaÃ§Ã£o
- ConfiguraÃ§Ã£o do BI

---

**Nota**: Este projeto estÃ¡ em desenvolvimento ativo seguindo o fluxo documentado. Para acompanhar o progresso, consulte o arquivo `fluxo de desenvolvimento.txt`.

## ğŸ“„ LicenÃ§a
MIT License.